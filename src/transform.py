import os
import pandas as pd
from tqdm import tqdm
import numpy as np
from src.logger import get_logger
from src.extract import concatenate_filtered_data

logger = get_logger()
# Step 1: Function to add the 'pickup_hour' column and group data by 'pickup_hour' and 'PULocationID'
def process_filtered_dataframe(df) -> pd.DataFrame:
    """
    Process a filtered DataFrame by adding a 'pickup_hour' column and grouping the data by 'pickup_hour' and 'PULocationID'.

    Args:

    - df: A pandas DataFrame containing the filtered data with columns 'pickup_datetime' and 'PULocationID'.

    Returns:

    - A DataFrame with the number of rides for each 'pickup_hour' and 'PULocationID'.

    """
    # Add a column for the pickup hour
    df['pickup_hour'] = df['pickup_datetime'].dt.floor('h')

    # Group by pickup hour and PULocationID, count the number of rides
    df_grouped = df.groupby(['pickup_hour', 'PULocationID']).size().reset_index(name='rides')
    
    return df_grouped

# Step 2: Function to add missing slots (time series transformation)
def add_missing_slots(df_grouped) -> pd.DataFrame:
    """
    Add missing slots to the time series data by filling in zeros for missing hours.

    Args:
    - df_grouped: A DataFrame with the number of rides for each 'pickup_hour' and 'PULocationID'.

    Returns:
    - A DataFrame with all missing hours filled in with zeros for each 'PULocationID'.
    """
    location_ids = df_grouped['PULocationID'].unique()
    full_range = pd.date_range(
        start=df_grouped['pickup_hour'].min(), end=df_grouped['pickup_hour'].max(), freq='h'
    )
    
    output_list = []  # Use a list for better performance
    
    for location_id in tqdm(location_ids):
        # Filter only rides for this location
        df_location = df_grouped.loc[df_grouped['PULocationID'] == location_id, ['pickup_hour', 'rides']]
        
        # Add missing dates with 0 in rides
        df_location = df_location.set_index('pickup_hour').reindex(full_range).fillna(0).reset_index()
        df_location = df_location.rename(columns={'index': 'pickup_hour'})  # Rename the reindexed column
        
        # Add the location ID back
        df_location['PULocationID'] = location_id
        
        # Append to the list instead of concatenating in each iteration
        output_list.append(df_location)
    
    # Concatenate all at once
    output = pd.concat(output_list, ignore_index=True)

    # Ensure the DataFrame is well-formed and reset index correctly
    output = output.reset_index(drop=True)
    
    return output

def transform_to_time_series_data(path,logger):
    """
    Transforms filtered data for a specific path into time-series format by
    concatenating, filling missing slots, and grouping data.

    Args:
        path (str): Identifier for the dataset.
        logger: Logger instance for logging info and errors.

    Returns:
        pd.DataFrame: Transformed time-series DataFrame.
    """
    # Concatenate filtered data
    df_filtered = concatenate_filtered_data(path)

    # Process and group data
    df_grouped = add_missing_slots(process_filtered_dataframe(df_filtered))
    logger.info(f"Data transformed to time-series format for {path}")
    
    return df_grouped


# Step 3: Function to generate the feature matrix and target vector
def get_cutoff_indices(df: pd.DataFrame, n_features: int, step_size: int = 1) -> list:
    """
    Generate cutoff indices for creating features and targets from time series data.
    
    Args:
    - df: DataFrame with time series data.
    - n_features: Number of time steps to use as features.
    - step_size: Number of steps to shift the window after each iteration.

    Returns:
    - List of tuples with indices for each sliding window (start, mid, end).
    """
    
    stop_position = len(df) - 1  # Define o limite para a última posição
    subseq_first_idx = 0
    subseq_mid_idx = n_features
    subseq_last_idx = n_features + 1
    indices = []

    # Loop para gerar os índices de corte com controle de step_size
    while subseq_last_idx <= stop_position:
        indices.append((subseq_first_idx, subseq_mid_idx, subseq_last_idx))
        subseq_first_idx += step_size
        subseq_mid_idx += step_size
        subseq_last_idx += step_size

    return indices


# Step 4: Function to create the feature matrix and target vector
def create_feature_matrix_and_target(df, cutoff_indices):
    """
    Creates a feature matrix and a target vector from a DataFrame using sliding window indices.
    
    Args:
    - df: A pandas DataFrame containing the time series data (e.g., column 'rides').
    - cutoff_indices: A list of tuples (first_index, mid_index, last_index) generated by the `get_cutoff_indices()` function.

    Returns:
    - feature_matrix: A NumPy array where each row contains a sliding window of values.
    - target_vector: A NumPy array containing the target value (the value at `last_index` for each window).
    """
    
    n_examples = len(cutoff_indices)
    n_features = cutoff_indices[0][1] - cutoff_indices[0][0]  # Número de recursos na janela

    # allocate matrix and vector
    feature_matrix = np.ndarray((n_examples, n_features), dtype=np.float32)
    target_vector = np.ndarray((n_examples,), dtype=np.float32)

    for i, (first_index, mid_index, last_index) in enumerate(cutoff_indices):
        feature_matrix[i, :] = df['rides'].iloc[first_index:mid_index].values
        target_vector[i] = df['rides'].iloc[last_index]

    return feature_matrix, target_vector


# Step 5: Function to process a parquet file by PULocationID
def process_feature_target_by_PULocationID(df, n_features, step_size=1):
    """
    Process data by PULocationID and apply sliding window transformation for each PULocationID.
    
    Args:
    - df: DataFrame containing the time series data with columns ['PULocationID', 'rides', 'pickup_hour'].
    - n_features: Number of previous time steps to use as features.
    - step_size: Step size for sliding window.
    
    Returns:
    - A DataFrame with PULocationID, features, and target.
    """
    
    assert set(df.columns) == {'pickup_hour', 'rides', 'PULocationID'}

    unique_pulocation_ids = df['PULocationID'].unique()
    features = pd.DataFrame()
    targets = pd.DataFrame()

    logger.info(f"Processing {len(unique_pulocation_ids)} unique PULocationIDs...")
    
    for pulocation_id in tqdm(unique_pulocation_ids):
        df_location = df[df['PULocationID'] == pulocation_id].sort_values('pickup_hour')
         # keep only ts data for this `location_id`
        df_location = df_location.loc[
            df_location['PULocationID'] == pulocation_id, 
            ['pickup_hour', 'rides']
        ]

        if len(df_location) < n_features:
            continue

        # Get cut indices with `step_size`
        cutoff_indices = get_cutoff_indices(df_location, n_features, step_size)

        # Pre-allocate arrays for features and targets
        n_examples = len(cutoff_indices)
        feature_matrix = np.ndarray((n_examples, n_features), dtype=np.float32)
        target_vector = np.ndarray((n_examples,), dtype=np.float32)
        pickup_hours = []

        # Assembles `feature_matrix` and `target_vector`
        for i, (start_idx, mid_idx, end_idx) in enumerate(cutoff_indices):
            feature_matrix[i, :] = df_location.iloc[start_idx:mid_idx]['rides'].values
            target_vector[i] = df_location.iloc[end_idx]['rides']
            pickup_hours.append(df_location.iloc[mid_idx]['pickup_hour'])

        # Creates DataFrames of features and targets
        df_location = pd.DataFrame(
            feature_matrix,
            columns=[f'rides_previous_{i+1}' for i in reversed(range(n_features))]
        )
        df_location['pickup_hour'] = pickup_hours
        df_location['PULocationID'] = pulocation_id

        # numpy -> pandas
        target_vector = pd.DataFrame(target_vector, columns=[f'target_rides_next_hour'])

        # Concatenates features and target
        features = pd.concat([features, df_location])
        targets = pd.concat([targets, target_vector])

    features.reset_index(inplace=True, drop=True)
    targets.reset_index(inplace=True, drop=True)

    return features, targets['target_rides_next_hour']
