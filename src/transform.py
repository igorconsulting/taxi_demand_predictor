import os
import pandas as pd
from tqdm import tqdm
import numpy as np

from src.paths import FILTERED_DATA_DIR, TRANSFORMED_DATA_DIR, TIME_SERIES_DATA_DIR

# Step 1: Function to add the 'pickup_hour' column and group data by 'pickup_hour' and 'PULocationID'
def process_filtered_dataframe(df):
    # Add a column for the pickup hour
    df['pickup_hour'] = df['pickup_datetime'].dt.floor('h')

    # Group by pickup hour and PULocationID, count the number of rides
    df_grouped = df.groupby(['pickup_hour', 'PULocationID']).size().reset_index(name='rides')
    
    return df_grouped

# Step 2: Function to add missing slots (time series transformation)
def add_missing_slots(df_grouped) -> pd.DataFrame:
    location_ids = df_grouped['PULocationID'].unique()
    full_range = pd.date_range(
        start=df_grouped['pickup_hour'].min(), end=df_grouped['pickup_hour'].max(), freq='h'
    )
    
    output_list = []  # Use a list for better performance
    
    for location_id in tqdm(location_ids):
        # Filter only rides for this location
        df_location = df_grouped.loc[df_grouped['PULocationID'] == location_id, ['pickup_hour', 'rides']]
        
        # Add missing dates with 0 in rides
        df_location = df_location.set_index('pickup_hour').reindex(full_range).fillna(0).reset_index()
        df_location = df_location.rename(columns={'index': 'pickup_hour'})  # Rename the reindexed column
        
        # Add the location ID back
        df_location['PULocationID'] = location_id
        
        # Append to the list instead of concatenating in each iteration
        output_list.append(df_location)
    
    # Concatenate all at once
    output = pd.concat(output_list, ignore_index=True)

    # Ensure the DataFrame is well-formed and reset index correctly
    output = output.reset_index(drop=True)
    
    return output

# Step 3: Function to process all parquet files in a folder
def process_all_filtered_files(input_dir=FILTERED_DATA_DIR, output_dir=TRANSFORMED_DATA_DIR):
    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)

    # Loop through each file in the input directory
    for filename in os.listdir(input_dir):
        if filename.endswith('.parquet'):
            output_file = f"transformed_{filename}"
            output_path = os.path.join(output_dir, output_file)
            
            # Check if the transformed file already exists
            if os.path.exists(output_path):
                print(f"{output_file} already exists. Skipping processing.")
                continue  # Skip this file if already processed

            file_path = os.path.join(input_dir, filename)
            print(f"Processing file: {filename}")

            # Read the parquet file
            df = pd.read_parquet(file_path)
            
            # Process the dataframe (add pickup_hour, group by, and add missing slots)
            df_grouped = process_filtered_dataframe(df)
            complete_df_grouped = add_missing_slots(df_grouped)

            # Save the transformed dataframe as a parquet file in the output directory
            complete_df_grouped.to_parquet(output_path)
            print(f"Saved transformed file: {output_file}")


# Step 4: Function to generate the feature matrix and target vector
def get_cutoff_indices(df: pd.DataFrame, n_features: int) -> list:
    """
    Get the indices of the cutoffs for each feature in the dataframe
    """
    df_len = len(df)
    cutoff_indices = []

    
    for i in range(df_len-n_features):
        first_index = i
        last_index = i + n_features-1
        next_index = i + n_features
        cutoff_indices.append((first_index, last_index, next_index))

    return cutoff_indices


# Step 5: Function to create the feature matrix and target vector
def create_feature_matrix_and_target(df, cutoff_indices):
    """
    Creates a feature matrix and a target vector from a DataFrame using sliding window indices.
    
    Args:
    - df: A pandas DataFrame containing the time series data (e.g., column 'rides').
    - cutoff_indices: A list of tuples (first_index, last_index, next_index) generated by the `get_cutoff_indices()` function.
    - n_features: The number of previous time steps to use as features (sliding window size).
    
    Returns:
    - feature_matrix: A NumPy array where each row contains a sliding window values.
    - target_vector: A NumPy array containing the target value (the value at `next_index` for each window).
    """
    
    # Generate the feature matrix using list comprehension
    feature_matrix = np.array([
        df['rides'].iloc[first_index:last_index+1].values 
        for first_index, last_index, _ in tqdm(cutoff_indices)
    ])
    
    # Generate the target vector using list comprehension
    target_vector = np.array([
        df['rides'].iloc[next_index] for _, _, next_index in tqdm(cutoff_indices)
    ])
    
    return feature_matrix, target_vector


# Step 6: Function to process a parquet file by PULocationID
def process_feature_target_by_PULocationID(file_path, n_features):
    """
    Process a parquet file, segmenting the data by PULocationID, and apply sliding window transformation
    for each PULocationID. Outputs a single DataFrame with PULocationID, features, and target.
    
    Args:
    - file_path: Path to the parquet file containing data.
    - n_features: The number of previous time steps to use as features (sliding window size).
    
    Returns:
    - A single DataFrame with PULocationID, features (feature_0 to feature_{n_features-1}), and target.
    """
    # Load the parquet file
    df = pd.read_parquet(file_path)

    # Get unique PULocationIDs
    unique_pulocation_ids = df['PULocationID'].unique()
    
    # List to store the results
    rows = []

    # Process each PULocationID separately
    for pulocation_id in tqdm(unique_pulocation_ids):
        # Filter the dataframe for this specific PULocationID
        df_location = df[df['PULocationID'] == pulocation_id].sort_values('pickup_hour')

        # Generate cutoff indices for the time series
        cutoff_indices = get_cutoff_indices(df_location, n_features=n_features)
        
        if len(cutoff_indices) == 0:
            # Skip if there's not enough data to generate the feature matrix
            continue
        
        # Create feature matrix and target vector
        feature_matrix, target_vector = create_feature_matrix_and_target(df_location, cutoff_indices)
        
        # Append the PULocationID, features, and target to the result list
        for i in range(feature_matrix.shape[0]):
            row = [pulocation_id] + list(feature_matrix[i]) + [target_vector[i]]
            rows.append(row)

    # Create a DataFrame with the collected rows
    columns = ['PULocationID'] + [f'feature_{i}' for i in range(n_features)] + ['target']
    final_df = pd.DataFrame(rows, columns=columns)
    
    return final_df


# Step 7: Function to process all transformed files
def process_all_transformed_files(input_dir=TRANSFORMED_DATA_DIR, output_dir=TIME_SERIES_DATA_DIR, n_features=24):
    """
    Process all files in the 'data/transformed' folder that follow the naming pattern
    'transformed_filtered_{type}_{year}-{month}.parquet' and save them to 'data/time_series' 
    with the name 'ts_{type}_{year}-{month}.parquet'.
    
    Args:
    - input_dir: Directory where the transformed parquet files are located.
    - output_dir: Directory where the time series parquet files will be saved.
    - n_features: Number of previous time steps to use as features (default is 24).
    
    Returns:
    - None: The function saves the processed files in the output directory.
    """
    # Ensure the output directory exists
    os.makedirs(output_dir, exist_ok=True)
    
    # Iterate over all parquet files in the input directory
    for filename in os.listdir(input_dir):
        if filename.endswith('.parquet') and filename.startswith('transformed_filtered_'):
            try:
                # Remove the prefix and suffix to get the core file name
                file_core = filename.replace('transformed_filtered_', '').replace('.parquet', '')

                # Use the last underscore to separate the type from the year-month
                data_type, year_month = file_core.rsplit('_', 1)
                
                # Extract year and month from the year_month string
                year, month = year_month.split('-')
                # Create the new filename following the pattern ts_{type}_{year}-{month}.parquet
                output_filename = f'ts_{data_type}_{year}-{month}.parquet'
                output_path = os.path.join(output_dir, output_filename)

                # Skip the file if it already exists
                if os.path.exists(output_path):
                    print(f"{output_filename} already exists. Skipping.")
                    continue

                # Full path to the current input file
                input_path = os.path.join(input_dir, filename)

                # Process the file to get the time series data for each PULocationID
                print(f"Processing {filename}...")
                final_df = process_feature_target_by_PULocationID(input_path, n_features)

                # Save the processed DataFrame as a parquet file
                final_df.to_parquet(output_path)
                print(f"Saved {output_filename} to {output_dir}")

            except Exception as e:
                print(f"Error processing {filename}: {e}")